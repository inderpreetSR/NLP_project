{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"---er.pdf\"\n",
    "\n",
    "open_filename = open(filename, 'rb')\n",
    "\n",
    "ind_manifesto = PyPDF2.PdfFileReader(open_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files that have to be merged one by one\n",
    "pdf1File = open('1. ----2014.pdf', 'rb')\n",
    "pdf2File = open('------2015.pdf', 'rb')\n",
    "pdf3File=open('-----2015.pdf', 'rb')\n",
    "pdf4File=open('--- Jun-19 ----er.pdf', 'rb')\n",
    "pdf5File=open('N------uestions.pdf', 'rb')\n",
    "pdf6File=open('NET ----on Paper.pdf', 'rb')\n",
    " \n",
    "# Read the files that you have opened\n",
    "pdf1Reader = PyPDF2.PdfFileReader(pdf1File)\n",
    "pdf2Reader = PyPDF2.PdfFileReader(pdf2File)\n",
    "pdf3Reader= PyPDF2.PdfFileReader(pdf2File)\n",
    "pdf4Reader= PyPDF2.PdfFileReader(pdf2File)\n",
    "pdf5Reader= PyPDF2.PdfFileReader(pdf2File)\n",
    "pdf6Reader= PyPDF2.PdfFileReader(pdf2File)\n",
    "\n",
    "# Create a new PdfFileWriter object which represents a blank PDF document\n",
    "pdfWriter = PyPDF2.PdfFileWriter()\n",
    " \n",
    "# Loop through all the pagenumbers for the first document\n",
    "for pageNum in range(pdf1Reader.numPages):\n",
    "    pageObj = pdf1Reader.getPage(pageNum)\n",
    "    pdfWriter.addPage(pageObj)\n",
    "# Loop through all the pagenumbers for the second document\n",
    "for pageNum in range(pdf2Reader.numPages):\n",
    "    pageObj = pdf2Reader.getPage(pageNum)\n",
    "    pdfWriter.addPage(pageObj)\n",
    "    \n",
    "# Loop through all the pagenumbers for the 3rd document\n",
    "for pageNum in range(pdf1Reader.numPages):\n",
    "    pageObj = pdf1Reader.getPage(pageNum)\n",
    "    pdfWriter.addPage(pageObj)\n",
    "     \n",
    "# Loop through all the pagenumbers for the 4 document\n",
    "for pageNum in range(pdf1Reader.numPages):\n",
    "    pageObj = pdf1Reader.getPage(pageNum)\n",
    "    pdfWriter.addPage(pageObj)\n",
    "\n",
    "# Loop through all the pagenumbers for the 5 document \n",
    "for pageNum in range(pdf1Reader.numPages):\n",
    "    pageObj = pdf1Reader.getPage(pageNum)\n",
    "    pdfWriter.addPage(pageObj)\n",
    "\n",
    "\n",
    "# Loop through all the pagenumbers for the 6 document\n",
    "for pageNum in range(pdf1Reader.numPages):\n",
    "    pageObj = pdf1Reader.getPage(pageNum)\n",
    "    pdfWriter.addPage(pageObj)\n",
    "    \n",
    "\n",
    "# Now that you have copied all the pages in both the documents, write them into the a new document\n",
    "pdfOutputFile = open('MergedFiles.pdf', 'wb')\n",
    "pdfWriter.write(pdfOutputFile)\n",
    " \n",
    "# Close all the files - Created as well as opened\n",
    "pdfOutputFile.close()\n",
    "pdf1File.close()\n",
    "pdf2File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_manifesto.getDocumentInfo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"MergedFiles.pdf\"\n",
    "\n",
    "open_filename = open(filename, 'rb')\n",
    "\n",
    "test_ind = PyPDF2.PdfFileReader(open_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ind.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = test_ind.numPages\n",
    "total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=ind_manifesto.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "text  = ''\n",
    "\n",
    "# Lets loop through, to read each page from the pdf file\n",
    "while(count < total_pages):\n",
    "    # Get the specified number of pages in the document\n",
    "    mani_page  = test_ind.getPage(count)\n",
    "    # Process the next page\n",
    "    count += 1\n",
    "    # Extract the text from the page\n",
    "    text += mani_page.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text != '':\n",
    "    text = text\n",
    "    \n",
    "else:\n",
    "    textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def to_lower(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specll check the words\n",
    "    #spell  = Speller(lang='en')\n",
    "    \n",
    "    #texts = spell(text)\n",
    "    \n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
    "\n",
    "lower_case = to_lower(text)\n",
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lower_case):\n",
    "    # split text phrases into words\n",
    "    words  = nltk.word_tokenize(lower_case)\n",
    "    \n",
    "    \n",
    "    # Create a list of all the punctuations we wish to remove\n",
    "    punctuations = ['.', ',', '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n",
    "    \n",
    "    #removing non related words\n",
    "    nrel_wrd=['new', 'email', 'phone', 'website', 'khas']\n",
    "\n",
    "       \n",
    "    # Remove all the special characters\n",
    "    punctuations = re.sub(r'\\W',' ', str(lower_case))\n",
    "    \n",
    "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
    "    stop_words  = stopwords.words('english')\n",
    "    stop_words.extend(nrel_wrd)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Getting rid of all the words that contain numbers in them\n",
    "    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n",
    "    \n",
    "    # remove all single characters\n",
    "    lower_case = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', lower_case)\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Removing non-english characters\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    # Return keywords which are not in stop words \n",
    "    keywords = [word for word in words if not word in stop_words  and word in punctuations ]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in clean_text(lower_case)]\n",
    "\n",
    "# lets print out the output from our function above and see how the data looks like\n",
    "clean_data = ' '.join(lemmatized_word)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([clean_data])\n",
    "df.columns = ['script']\n",
    "df.index = ['Itula']\n",
    "df\n",
    "corpus = df.script\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "data_vect = vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "data_vect_feat = pd.DataFrame(data_vect.toarray(), columns=feature_names)\n",
    "data_vect_feat.index = df.index\n",
    "data_vect_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_vect_feat.transpose()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Find the top 1000 words written in the manifesto\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "    \n",
    "for x in list(top_dict)[0:100]:\n",
    "    print(\"key {}, value {} \".format(x,  top_dict[x]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 100 words for each comedian\n",
    "words = []\n",
    "for president in data:\n",
    "    top = [word for (word, count) in top_dict[president]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Image used in which our world cloud output will be\n",
    "img1 = imageio.imread(\"abpic.jpg\")\n",
    "hcmask1 = img1\n",
    "\n",
    "# Get 100 words based on the \n",
    "words_except_stop_dist = nltk.FreqDist(w for w in words[:100]) \n",
    "wordcloud = WordCloud(stopwords=set(STOPWORDS),background_color='black',mask=hcmask1).generate(\" \".join(words_except_stop_dist))\n",
    "plt.imshow(wordcloud,interpolation = 'bilinear')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,12)\n",
    "plt.axis('off')\n",
    "plt.title(\"Top most common 100 words from Dr. Itula's Manifesto 2019\",fontsize=20)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('Manifesto_top_100.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
